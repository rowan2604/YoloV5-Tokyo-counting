{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "615dae68",
   "metadata": {},
   "source": [
    "## exemple test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b8fa4eff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import cv2\n",
    "# import numpy as np\n",
    "\n",
    "# # Load YOLOv5 model\n",
    "# model = torch.hub.load('../yolov5','custom', source='local', path=\"./yolov5/yolov5s.pt\")\n",
    "# # Load input video\n",
    "# cap = cv2.VideoCapture('londres.mp4')\n",
    "\n",
    "# # Set up output video writer\n",
    "# width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "# height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "# fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "# fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "# out = cv2.VideoWriter('output_video.mp4', fourcc, fps, (width, height))\n",
    "\n",
    "# # Process video frame-by-frame\n",
    "# while cap.isOpened():\n",
    "#     ret, frame = cap.read()\n",
    "#     if not ret:\n",
    "#         break\n",
    "\n",
    "#     # Run YOLOv5 on the frame\n",
    "#     results = model(frame)\n",
    "\n",
    "#     # Draw bounding boxes around detected objects\n",
    "#     for result in results.xyxy[0]:\n",
    "#         x1, y1, x2, y2, conf, cls = result\n",
    "#         cv2.rectangle(frame, (int(x1), int(y1)), (int(x2), int(y2)), (0, 255, 0), 2)\n",
    "#         cv2.putText(frame, f'{model.names[int(cls)]} {conf:.2f}', (int(x1), int(y1) - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "\n",
    "#     # Save frame to output video\n",
    "#     out.write(frame)\n",
    "\n",
    "#     # Display frame\n",
    "#     cv2.imshow('frame', frame)\n",
    "#     if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "#         break\n",
    "\n",
    "# # Release resources\n",
    "# cap.release()\n",
    "# out.release()\n",
    "# cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fa99ca10",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[31m\u001b[1mrequirements:\u001b[0m YOLOv5 requirement \"torchvision>=0.8.1\" not found, attempting AutoUpdate...\n",
      "Requirement already satisfied: torchvision>=0.8.1 in c:\\users\\rowan\\anaconda\\lib\\site-packages (0.14.1)\n",
      "Requirement already satisfied: typing_extensions in c:\\users\\rowan\\anaconda\\lib\\site-packages (from torchvision>=0.8.1) (3.10.0.2)\n",
      "Requirement already satisfied: numpy in c:\\users\\rowan\\anaconda\\lib\\site-packages (from torchvision>=0.8.1) (1.20.3)\n",
      "Requirement already satisfied: requests in c:\\users\\rowan\\anaconda\\lib\\site-packages (from torchvision>=0.8.1) (2.26.0)\n",
      "Requirement already satisfied: torch==1.13.1 in c:\\users\\rowan\\anaconda\\lib\\site-packages (from torchvision>=0.8.1) (1.13.1)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\rowan\\anaconda\\lib\\site-packages (from torchvision>=0.8.1) (9.3.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\rowan\\anaconda\\lib\\site-packages (from requests->torchvision>=0.8.1) (1.26.6)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\rowan\\anaconda\\lib\\site-packages (from requests->torchvision>=0.8.1) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\rowan\\anaconda\\lib\\site-packages (from requests->torchvision>=0.8.1) (3.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\rowan\\anaconda\\lib\\site-packages (from requests->torchvision>=0.8.1) (2022.12.7)\n",
      "\n",
      "\u001b[31m\u001b[1mrequirements:\u001b[0m 1 package updated per C:\\Users\\rowan\\Desktop\\Projet_IA_M2\\yolov5\\requirements.txt\n",
      "\u001b[31m\u001b[1mrequirements:\u001b[0m  \u001b[1mRestart runtime or rerun command for updates to take effect\u001b[0m\n",
      "\n",
      "YOLOv5  2023-2-25 Python-3.8.11 torch-1.13.1 CPU\n",
      "\n",
      "Fusing layers... \n",
      "YOLOv5s summary: 213 layers, 7225885 parameters, 0 gradients\n",
      "Adding AutoShape... \n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import cv2\n",
    "\n",
    "\n",
    "cap=cv2.VideoCapture(\"london2.mp4\")\n",
    "\n",
    "#model = torch.hub.load('ultralytics/yolov5', 'yolov5s', pretrained=True)\n",
    "model = torch.hub.load('../yolov5','custom', source='local', path=\"./yolov5/yolov5s.pt\")\n",
    "\n",
    "\n",
    "b=model.names[2] = 'car'\n",
    "c=model.names[0] = 'person'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d0ceab3d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cx:  708 cx1:  921 cx2:  1209\n",
      "cy:  258 cy1:  400\n",
      "cx:  708 cx1:  921 cx2:  1209\n",
      "cy:  258 cy1:  400\n",
      "cx:  708 cx1:  921 cx2:  1209\n",
      "cy:  258 cy1:  400\n",
      "cx:  709 cx1:  921 cx2:  1209\n",
      "cy:  258 cy1:  400\n",
      "cx:  710 cx1:  921 cx2:  1209\n",
      "cy:  257 cy1:  400\n",
      "cx:  710 cx1:  921 cx2:  1209\n",
      "cy:  257 cy1:  400\n",
      "cx:  710 cx1:  921 cx2:  1209\n",
      "cy:  257 cy1:  400\n",
      "cx:  710 cx1:  921 cx2:  1209\n",
      "cy:  257 cy1:  400\n",
      "cx:  710 cx1:  921 cx2:  1209\n",
      "cy:  257 cy1:  400\n",
      "cx:  709 cx1:  921 cx2:  1209\n",
      "cy:  258 cy1:  400\n",
      "cx:  709 cx1:  921 cx2:  1209\n",
      "cy:  257 cy1:  400\n",
      "cx:  708 cx1:  921 cx2:  1209\n",
      "cy:  257 cy1:  400\n",
      "cx:  1523 cx1:  921 cx2:  1209\n",
      "cy:  129 cy1:  400\n",
      "cx:  709 cx1:  921 cx2:  1209\n",
      "cy:  257 cy1:  400\n",
      "cx:  709 cx1:  921 cx2:  1209\n",
      "cy:  257 cy1:  400\n",
      "cx:  709 cx1:  921 cx2:  1209\n",
      "cy:  257 cy1:  400\n",
      "cx:  709 cx1:  921 cx2:  1209\n",
      "cy:  257 cy1:  400\n",
      "cx:  709 cx1:  921 cx2:  1209\n",
      "cy:  257 cy1:  400\n",
      "cx:  709 cx1:  921 cx2:  1209\n",
      "cy:  257 cy1:  400\n",
      "cx:  709 cx1:  921 cx2:  1209\n",
      "cy:  257 cy1:  400\n",
      "cx:  716 cx1:  921 cx2:  1209\n",
      "cy:  257 cy1:  400\n",
      "cx:  1485 cx1:  921 cx2:  1209\n",
      "cy:  130 cy1:  400\n",
      "cx:  717 cx1:  921 cx2:  1209\n",
      "cy:  257 cy1:  400\n",
      "cx:  1485 cx1:  921 cx2:  1209\n",
      "cy:  130 cy1:  400\n",
      "cx:  720 cx1:  921 cx2:  1209\n",
      "cy:  258 cy1:  400\n",
      "cx:  1478 cx1:  921 cx2:  1209\n",
      "cy:  129 cy1:  400\n",
      "cx:  722 cx1:  921 cx2:  1209\n",
      "cy:  258 cy1:  400\n",
      "cx:  1478 cx1:  921 cx2:  1209\n",
      "cy:  130 cy1:  400\n",
      "cx:  724 cx1:  921 cx2:  1209\n",
      "cy:  256 cy1:  400\n",
      "cx:  1475 cx1:  921 cx2:  1209\n",
      "cy:  129 cy1:  400\n",
      "cx:  716 cx1:  921 cx2:  1209\n",
      "cy:  257 cy1:  400\n",
      "cx:  1475 cx1:  921 cx2:  1209\n",
      "cy:  128 cy1:  400\n",
      "cx:  708 cx1:  921 cx2:  1209\n",
      "cy:  258 cy1:  400\n",
      "cx:  1479 cx1:  921 cx2:  1209\n",
      "cy:  128 cy1:  400\n",
      "cx:  707 cx1:  921 cx2:  1209\n",
      "cy:  257 cy1:  400\n",
      "cx:  1464 cx1:  921 cx2:  1209\n",
      "cy:  128 cy1:  400\n",
      "cx:  711 cx1:  921 cx2:  1209\n",
      "cy:  258 cy1:  400\n",
      "cx:  1461 cx1:  921 cx2:  1209\n",
      "cy:  128 cy1:  400\n",
      "cx:  711 cx1:  921 cx2:  1209\n",
      "cy:  258 cy1:  400\n",
      "cx:  1460 cx1:  921 cx2:  1209\n",
      "cy:  128 cy1:  400\n",
      "cx:  709 cx1:  921 cx2:  1209\n",
      "cy:  259 cy1:  400\n",
      "cx:  1456 cx1:  921 cx2:  1209\n",
      "cy:  129 cy1:  400\n",
      "cx:  710 cx1:  921 cx2:  1209\n",
      "cy:  259 cy1:  400\n",
      "cx:  1453 cx1:  921 cx2:  1209\n",
      "cy:  129 cy1:  400\n",
      "cx:  709 cx1:  921 cx2:  1209\n",
      "cy:  256 cy1:  400\n",
      "cx:  1450 cx1:  921 cx2:  1209\n",
      "cy:  129 cy1:  400\n",
      "cx:  712 cx1:  921 cx2:  1209\n",
      "cy:  256 cy1:  400\n",
      "cx:  1444 cx1:  921 cx2:  1209\n",
      "cy:  128 cy1:  400\n",
      "cx:  711 cx1:  921 cx2:  1209\n",
      "cy:  256 cy1:  400\n",
      "cx:  1444 cx1:  921 cx2:  1209\n",
      "cy:  128 cy1:  400\n",
      "cx:  709 cx1:  921 cx2:  1209\n",
      "cy:  253 cy1:  400\n",
      "cx:  1424 cx1:  921 cx2:  1209\n",
      "cy:  130 cy1:  400\n",
      "cx:  715 cx1:  921 cx2:  1209\n",
      "cy:  252 cy1:  400\n",
      "cx:  1427 cx1:  921 cx2:  1209\n",
      "cy:  130 cy1:  400\n",
      "cx:  710 cx1:  921 cx2:  1209\n",
      "cy:  254 cy1:  400\n",
      "cx:  1419 cx1:  921 cx2:  1209\n",
      "cy:  130 cy1:  400\n",
      "cx:  712 cx1:  921 cx2:  1209\n",
      "cy:  255 cy1:  400\n",
      "cx:  1409 cx1:  921 cx2:  1209\n",
      "cy:  130 cy1:  400\n",
      "cx:  711 cx1:  921 cx2:  1209\n",
      "cy:  255 cy1:  400\n",
      "cx:  1411 cx1:  921 cx2:  1209\n",
      "cy:  130 cy1:  400\n",
      "cx:  702 cx1:  921 cx2:  1209\n",
      "cy:  255 cy1:  400\n",
      "cx:  1403 cx1:  921 cx2:  1209\n",
      "cy:  130 cy1:  400\n",
      "cx:  704 cx1:  921 cx2:  1209\n",
      "cy:  256 cy1:  400\n",
      "cx:  1401 cx1:  921 cx2:  1209\n",
      "cy:  130 cy1:  400\n",
      "cx:  706 cx1:  921 cx2:  1209\n",
      "cy:  257 cy1:  400\n",
      "cx:  1398 cx1:  921 cx2:  1209\n",
      "cy:  130 cy1:  400\n",
      "cx:  707 cx1:  921 cx2:  1209\n",
      "cy:  257 cy1:  400\n",
      "cx:  1398 cx1:  921 cx2:  1209\n",
      "cy:  130 cy1:  400\n",
      "cx:  708 cx1:  921 cx2:  1209\n",
      "cy:  261 cy1:  400\n",
      "cx:  1389 cx1:  921 cx2:  1209\n",
      "cy:  130 cy1:  400\n",
      "cx:  708 cx1:  921 cx2:  1209\n",
      "cy:  258 cy1:  400\n",
      "cx:  1388 cx1:  921 cx2:  1209\n",
      "cy:  130 cy1:  400\n"
     ]
    }
   ],
   "source": [
    "cap=cv2.VideoCapture(\"london2.mp4\")\n",
    "\n",
    "width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "\n",
    "out = cv2.VideoWriter('output_video2.mp4', fourcc, fps, (width, height)) #lieu d'enregistrement de la vidéo\n",
    "\n",
    "size=416 #taille du model\n",
    "\n",
    "counter=0 #compteur voiture\n",
    "counter_person=0\n",
    "\n",
    "cy1=400 #hauteur en y de la premiere ligne (du haut vers le bas +400 px)\n",
    "# cy2=800\n",
    "\n",
    "cx1=int(width*0.48) #coo x a gauche  de la premiere ligne\n",
    "cx2=int(width*0.63) #coo x a droite \n",
    "\n",
    "offset=1\n",
    "\n",
    "#  batch = [cap.read()[0] for _ in range(1)]\n",
    "#     if  not (batch):\n",
    "#         break\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret,img=cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "#     batch = [cap.read()[0] for _ in range(1)]\n",
    "#     if  not (batch):\n",
    "#         break\n",
    "    img=cv2.resize(img,(width,height)) #prendre pourcentage des pixels \n",
    "    \n",
    "    cx3=cx1-80\n",
    "    cx4=cx2-280\n",
    "    cy2=cy1-50\n",
    "\n",
    "    cx5=cx1-10\n",
    "    cx6=cx2+100\n",
    "    cy3=cy1+100\n",
    "    \n",
    "    cv2.line(img,(cx3,cy2),(cx4,cy2),(0,255,0),2) #line 2 persons\n",
    "\n",
    "    cv2.line(img,(cx1,cy1-20),(cx2,cy1-20),(0,0,255),2) #line 1 vehicule\n",
    "    \n",
    "    cv2.line(img,(cx5,cy3),(cx6,cy3),(0,0,255),2) #line 2 vehicule\n",
    "    \n",
    "    results=model(img,size)\n",
    "    a=results.pandas().xyxy[0]\n",
    "    #print(a)\n",
    "    \n",
    "    for index, row in results.pandas().xyxy[0].iterrows():\n",
    "        \n",
    "        x1=int(row['xmin'])\n",
    "        y1=int(row['ymin'])\n",
    "        x2=int(row['xmax'])\n",
    "        y2=int(row['ymax'])\n",
    "        \n",
    "        d=(row['class'])\n",
    "        conf=(row['confidence'])\n",
    "        \n",
    "        if d==2:\n",
    "                if conf>0.5 :\n",
    "                    cv2.rectangle(img,(x1,y1),(x2,y2),(0,255,0),1)\n",
    "                    rectx1,recty1=((x1+x2)/2,(y1+y2)/2)\n",
    "                    rectcenter=int(rectx1),int(recty1)\n",
    "#                     cx=rectcenter[0]\n",
    "#                     cy=rectcenter[1]\n",
    "                    cx=int((x1+x2)*0.5)\n",
    "                    \n",
    "                    cy=int((y1+y2)/4)\n",
    "                    print(\"cx: \",cx,\"cx1: \",cx1,\"cx2: \",cx2)\n",
    "                    print(\"cy: \",cy,\"cy1: \",cy1)\n",
    "                    cv2.circle(img,(cx,cy),1,(255,0,0),10)\n",
    "                    #cv2.putText(img,str(conf),(x1,y1),cv2.FONT_HERSHEY_PLAIN,2,(255,255,255),2)\n",
    "                    \n",
    "\n",
    "                if cy<(cy1+offset) and cy>(cy1-offset) :#and cx<(cx2+offset) and cx>(cx1-offset): \n",
    "#                         cv2.waitKey(0)\n",
    "#                         cv2.imshow(\"frame\",img+55)                   \n",
    "                        counter+=1\n",
    "#                         batch = [cap.read()[0] for _ in range(50)]\n",
    "#                         if  not (batch):\n",
    "#                              break\n",
    "                        cv2.line(img,(cx1,cy1-20),(cx2,cy1-20),(0,255,0),2)\n",
    "                        #cv2.line(img,(200,cy1),(400,cy1),(0,255,127),2)\n",
    "                        print(counter, \"au milieu\")\n",
    "                        \n",
    "                if cy<(cy2+offset) and cy>(cy2-offset) and cx<(cx3+offset) and cx>(cx4-offset):                    \n",
    "                        counter+=1\n",
    "#                         batch = [cap.read()[0] for _ in range(2)]\n",
    "#                         if  not (batch):\n",
    "#                             break\n",
    "                        cv2.line(img,(cx3,cy2),(cx4,cy2),(0,255,0),2) # couting visu\n",
    "                        print(counter, \"en bas la \")\n",
    "                cv2.putText(img,str(counter),(x2,y2),cv2.FONT_HERSHEY_PLAIN,2,(255,255,255),2)\n",
    "        if d==0:\n",
    "            if conf>0.5 :\n",
    "                cv2.rectangle(img,(x1,y1),(x2,y2),(0,0,255),1)\n",
    "                rectx1,recty1=((x1+x2)/2,(y1+y2)/2)\n",
    "                rectcenter=int(rectx1),int(recty1)\n",
    "                cx=rectcenter[0]\n",
    "                cy=rectcenter[1]\n",
    "                cv2.circle(img,(x1,y1),1,(255,0,0),2)\n",
    "\n",
    "                if cy<(cy3+offset) and cy>(cy3-offset) and cx<(cx5+offset) and cx>(cx6-offset):                    \n",
    "                            counter_person+=1\n",
    "#                             batch = [cap.read()[0] for _ in range(2)]\n",
    "#                             if  not (batch):\n",
    "#                                 break\n",
    "                            cv2.line(img,(cx5,cy3),(cx6,cy3),(0,0,255),2) #line 2 persons visu\n",
    "                            print(counter_person, \"persons\")\n",
    "                cv2.putText(img,str(counter_person),(x1,y1),cv2.FONT_HERSHEY_PLAIN,2,(255,0,0),2)\n",
    "    # Save frame to output video\n",
    "    out.write(img)\n",
    "    cv2.imshow(\"frame\",img)\n",
    "    if cv2.waitKey(1) &0xFF==27: #== ord('q'):\n",
    "        break\n",
    "cap.release()\n",
    "out.release()\n",
    "cv2.destroyAllWindows()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "64dc798b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[31m\u001b[1mrequirements:\u001b[0m YOLOv5 requirement \"torchvision>=0.8.1\" not found, attempting AutoUpdate...\n",
      "Requirement already satisfied: torchvision>=0.8.1 in c:\\users\\rowan\\anaconda\\lib\\site-packages (0.14.1)\n",
      "Requirement already satisfied: typing_extensions in c:\\users\\rowan\\anaconda\\lib\\site-packages (from torchvision>=0.8.1) (3.10.0.2)\n",
      "Requirement already satisfied: numpy in c:\\users\\rowan\\anaconda\\lib\\site-packages (from torchvision>=0.8.1) (1.20.3)\n",
      "Requirement already satisfied: requests in c:\\users\\rowan\\anaconda\\lib\\site-packages (from torchvision>=0.8.1) (2.26.0)\n",
      "Requirement already satisfied: torch==1.13.1 in c:\\users\\rowan\\anaconda\\lib\\site-packages (from torchvision>=0.8.1) (1.13.1)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\rowan\\anaconda\\lib\\site-packages (from torchvision>=0.8.1) (9.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\rowan\\anaconda\\lib\\site-packages (from requests->torchvision>=0.8.1) (2022.12.7)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\rowan\\anaconda\\lib\\site-packages (from requests->torchvision>=0.8.1) (1.26.6)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\rowan\\anaconda\\lib\\site-packages (from requests->torchvision>=0.8.1) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\rowan\\anaconda\\lib\\site-packages (from requests->torchvision>=0.8.1) (3.2)\n",
      "\n",
      "\u001b[31m\u001b[1mrequirements:\u001b[0m 1 package updated per C:\\Users\\rowan\\Desktop\\Projet_IA_M2\\yolov5\\requirements.txt\n",
      "\u001b[31m\u001b[1mrequirements:\u001b[0m  \u001b[1mRestart runtime or rerun command for updates to take effect\u001b[0m\n",
      "\n",
      "YOLOv5  2023-2-25 Python-3.8.11 torch-1.13.1 CPU\n",
      "\n",
      "Fusing layers... \n",
      "YOLOv5s summary: 213 layers, 7225885 parameters, 0 gradients\n",
      "Adding AutoShape... \n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import cv2\n",
    "\n",
    "\n",
    "cap=cv2.VideoCapture(\"londre.mp4\")\n",
    "\n",
    "#model = torch.hub.load('ultralytics/yolov5', 'yolov5s', pretrained=True)\n",
    "model = torch.hub.load('../yolov5','custom', source='local', path=\"./yolov5/yolov5s.pt\")\n",
    "\n",
    "b=model.names[2] = 'car'\n",
    "c=model.names[0] = 'person'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7458339c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Video width: 1920\n",
      "Video height: 1080\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'class'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m~\\anaconda\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mget_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3360\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3361\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3362\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda\\lib\\site-packages\\pandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m~\\anaconda\\lib\\site-packages\\pandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'class'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_15708/944154833.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 31\u001b[1;33m         \u001b[0md\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'class'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     32\u001b[0m         \u001b[0mconf\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'conf'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda\\lib\\site-packages\\pandas\\core\\series.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m    940\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    941\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mkey_is_scalar\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 942\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_value\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    943\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    944\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mis_hashable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda\\lib\\site-packages\\pandas\\core\\series.py\u001b[0m in \u001b[0;36m_get_value\u001b[1;34m(self, label, takeable)\u001b[0m\n\u001b[0;32m   1049\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1050\u001b[0m         \u001b[1;31m# Similar to Index.get_value, but we do not fall back to positional\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1051\u001b[1;33m         \u001b[0mloc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1052\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_values_for_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1053\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mget_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3361\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3362\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3363\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3364\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3365\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mis_scalar\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0misna\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhasnans\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'class'"
     ]
    }
   ],
   "source": [
    "width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "\n",
    "print(f\"Video width: {width}\")\n",
    "print(f\"Video height: {height}\")\n",
    "\n",
    "size=416\n",
    "\n",
    "counter=0\n",
    "\n",
    "cy1=400\n",
    "offset=5\n",
    "while True:\n",
    "    ret,img=cap.read()\n",
    "    \n",
    "    \n",
    "    img=cv2.resize(img,(width,height)) #prendre pourcentage des pixels taille de l'écran 960,540\n",
    "    cv2.line(img,(int(width*0.35),cy1),(int(width*0.6),cy1),(0,0,255),2)\n",
    "    results=model(img,size)\n",
    "    a=results.pandas().xyxy[0]\n",
    "    #print(a)\n",
    "\n",
    "    for index, row in df.iterrows():\n",
    "        \n",
    "        x1=int(row['x1'])\n",
    "        y1=int(row['y1'])\n",
    "        x2=int(row['x2'])\n",
    "        y2=int(row['y2'])\n",
    "        \n",
    "        \n",
    "        d=(row['class'])\n",
    "        conf=(row['conf'])\n",
    "        \n",
    "        if d==2:\n",
    "            #print(\"x_dist+ \",x_dist,\"y_dist+ \",y_dist,\"sum\",sum_dist)\n",
    "            #if sum_dist>140:\n",
    "                if conf>0.2 :\n",
    "                    cv2.rectangle(img,(x1,y1),(x2,y2),(0,255,0),2)\n",
    "                    rectx1,recty1=((x1+x2)/2,(y1+y2)/2)\n",
    "                    rectcenter=int(rectx1),int(recty1)\n",
    "                    cx=rectcenter[0]\n",
    "                    cy=rectcenter[1]\n",
    "                    cv2.circle(img,(cx,cy),2,(255,0,0),-1)\n",
    "                    #cv2.putText(img,str(conf),(x1,y1),cv2.FONT_HERSHEY_PLAIN,2,(255,255,255),2)\n",
    "                #if cy<(cy1+offset) and cy>(cy1-offset):\n",
    "                if cy<(cy1+offset) and cy>(cy1-offset):\n",
    "                    counter+=1\n",
    "                    cv2.line(img,(int(width*0.3),cy1),(int(width*0.5),cy1),(0,255,0),2)\n",
    "                    #cv2.line(img,(200,cy1),(400,cy1),(0,255,127),2)\n",
    "                    print(counter)\n",
    "                cv2.putText(img,str(counter),(x2,y2),cv2.FONT_HERSHEY_PLAIN,2,(255,255,255),2)\n",
    "        if d==0:\n",
    "            cv2.rectangle(img,(x1,y1),(x2,y2),(0,0,255),2)\n",
    "            rectx1,recty1=((x1+x2)/2,(y1+y2)/2)\n",
    "            rectcenter=int(rectx1),int(recty1)\n",
    "            cx=rectcenter[0]\n",
    "            cy=rectcenter[1]\n",
    "            cv2.circle(img,(cx,cy),3,(255,0,0),-1)\n",
    "            #cv2.putText(img,str(c),(x1,y1),cv2.FONT_HERSHEY_PLAIN,2,(255,0,0),2)\n",
    "    cv2.imshow(\"exemple\",img)\n",
    "    if cv2.waitKey(1) &0xFF==27: #== ord('q'):\n",
    "        break\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5752a347",
   "metadata": {},
   "source": [
    "## test philadelphia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "90564955",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[31m\u001b[1mrequirements:\u001b[0m YOLOv5 requirement \"torchvision>=0.8.1\" not found, attempting AutoUpdate...\n",
      "Requirement already satisfied: torchvision>=0.8.1 in c:\\users\\rowan\\anaconda\\lib\\site-packages (0.14.1)\n",
      "Requirement already satisfied: typing_extensions in c:\\users\\rowan\\anaconda\\lib\\site-packages (from torchvision>=0.8.1) (3.10.0.2)\n",
      "Requirement already satisfied: numpy in c:\\users\\rowan\\anaconda\\lib\\site-packages (from torchvision>=0.8.1) (1.20.3)\n",
      "Requirement already satisfied: requests in c:\\users\\rowan\\anaconda\\lib\\site-packages (from torchvision>=0.8.1) (2.26.0)\n",
      "Requirement already satisfied: torch==1.13.1 in c:\\users\\rowan\\anaconda\\lib\\site-packages (from torchvision>=0.8.1) (1.13.1)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\rowan\\anaconda\\lib\\site-packages (from torchvision>=0.8.1) (9.3.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\rowan\\anaconda\\lib\\site-packages (from requests->torchvision>=0.8.1) (1.26.6)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\rowan\\anaconda\\lib\\site-packages (from requests->torchvision>=0.8.1) (2022.12.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\rowan\\anaconda\\lib\\site-packages (from requests->torchvision>=0.8.1) (3.2)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\rowan\\anaconda\\lib\\site-packages (from requests->torchvision>=0.8.1) (2.0.4)\n",
      "\n",
      "\u001b[31m\u001b[1mrequirements:\u001b[0m 1 package updated per C:\\Users\\rowan\\Desktop\\Projet_IA_M2\\yolov5\\requirements.txt\n",
      "\u001b[31m\u001b[1mrequirements:\u001b[0m  \u001b[1mRestart runtime or rerun command for updates to take effect\u001b[0m\n",
      "\n",
      "YOLOv5  2023-2-25 Python-3.8.11 torch-1.13.1 CPU\n",
      "\n",
      "Fusing layers... \n",
      "YOLOv5s summary: 213 layers, 7225885 parameters, 0 gradients\n",
      "Adding AutoShape... \n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import cv2\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "cap=cv2.VideoCapture(\"londre.mp4\")\n",
    "\n",
    "#model = torch.hub.load('ultralytics/yolov5', 'yolov5s', pretrained=True)\n",
    "model = torch.hub.load('../yolov5','custom', source='local', path=\"./yolov5/yolov5s.pt\")\n",
    "\n",
    "\n",
    "b=model.names[2] = 'car'\n",
    "c=model.names[0] = 'person'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b2e2254b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "size=416\n",
    "\n",
    "count=0\n",
    "counter=0\n",
    "counter_person=0\n",
    "\n",
    "color=(0,0,255)\n",
    "cx1=390\n",
    "cy1=600\n",
    "cx2=370\n",
    "cy2=570\n",
    "x_offset=6\n",
    "y_offset=6\n",
    "tab=[]\n",
    "df = pd.DataFrame(columns=['x1', 'y1', 'x2', 'y2'])\n",
    "\n",
    "while True:\n",
    "    ret,img=cap.read()\n",
    "    batch = [cap.read()[0] for _ in range(1)]\n",
    "    if  not (batch):\n",
    "        break\n",
    "    count += 1\n",
    "    if count % 4 != 0:\n",
    "         continue\n",
    "    img=cv2.resize(img,(900,600)) #prendre pourcentage des pixels taille de l'écran 960,540\n",
    "    cv2.line(img,(cx1,cy1),(cx1,cy1-100),(0,0,255),2)\n",
    "    cv2.line(img,(cx2-40,cy2),(cx2,cy2-50),(0,0,255),3)\n",
    "\n",
    "\n",
    "    results=model(img,size)\n",
    "    a=results.pandas().xyxy[0]\n",
    "\n",
    "    for index, row in results.pandas().xyxy[0].iterrows():\n",
    "        \n",
    "        x1=int(row['xmin'])\n",
    "        y1=int(row['ymin'])\n",
    "        x2=int(row['xmax'])\n",
    "        y2=int(row['ymax'])\n",
    "        \n",
    "        x_dist=x2-x1\n",
    "        y_dist=y2-y1\n",
    "        sum_dist=x_dist+y_dist\n",
    "        \n",
    "        d=(row['class'])\n",
    "        conf=(row['confidence'])\n",
    "        \n",
    "        if d==2:\n",
    "                if conf>0.3 :\n",
    "                    #cv2.rectangle(img,(x1,y1),(x2,y2),(0,255,0),2)\n",
    "                    tab.append(cv2.rectangle(img,(x1,y1),(x2,y2),(0,255,0),2))#\"puis afficher a chaque frame\"\n",
    "                    df=df.append({'x1':x1,'y1':y1, 'x2':x2, 'y2':y2},ignore_index=True)\n",
    "                    rectx1,recty1=((x1+x2)/2,(y1+y2)/2)\n",
    "                    rectcenter=int(rectx1),int(recty1)\n",
    "                    cx=rectcenter[0]\n",
    "                    cy=rectcenter[1]\n",
    "                    cv2.circle(img,(cx,cy),2,(255,0,0),-1)\n",
    "                    #cv2.putText(img,str(conf),(x1,y1),cv2.FONT_HERSHEY_PLAIN,2,(255,255,255),2)\n",
    "                #if cy<(cy1+offset) and cy>(cy1-offset) and cx<(cx1+offset) and cx>(cx1-offset):\n",
    "                if cx<(cx1+x_offset) and cx>(cx1-x_offset) and cy<(cy1+y_offset):\n",
    "                    counter+=1\n",
    "                    cv2.line(img,(cx1,cy1),(cx1,cy1-100),(0,255,0),3)\n",
    "                    #cv2.line(img,(cx1+30,cy1),(cx1+30,cy1-100),(0,255,0),3)\n",
    "\n",
    "                    #cv2.line(img,(200,cy1),(400,cy1),(0,255,127),2)\n",
    "                    print(counter)\n",
    "                    #cv2.putText(img,str(counter),(x2,y2),cv2.FONT_HERSHEY_PLAIN,2,(255,255,255),2)\n",
    "        if d==0:\n",
    "            cv2.rectangle(img,(x1,y1),(x2,y2),(0,0,255),2)\n",
    "            rectx1,recty1=((x1+x2)/2,(y1+y2)/2)\n",
    "            rectcenter=int(rectx1),int(recty1)\n",
    "            cx=rectcenter[0]\n",
    "            cy=rectcenter[1]\n",
    "            cv2.circle(img,(cx,cy),3,(255,0,0),-1)\n",
    "            if cx<(cx2+x_offset) and cx>(cx2-x_offset) and cy<(cy2+y_offset):\n",
    "                counter_person+=1\n",
    "                cv2.line(img,(cx2-40,cy2),(cx2,cy2-50),(0,255,0),1)\n",
    "                print(counter_person)\n",
    "\n",
    "            #cv2.putText(img,str(c),(x1,y1),cv2.FONT_HERSHEY_PLAIN,2,(255,0,0),2)\n",
    "    cv2.imshow(\"philadelphia\",img)\n",
    "    if cv2.waitKey(1) == ord('q'): #echap sortir\n",
    "        break\n",
    "#print(df)\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53f13615",
   "metadata": {},
   "source": [
    "### Tokyo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "01ac2dbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\rowan/.cache\\torch\\hub\\ultralytics_yolov5_master\n",
      "YOLOv5  2023-2-7 Python-3.8.11 torch-1.13.1 CPU\n",
      "\n",
      "Fusing layers... \n",
      "YOLOv5s summary: 213 layers, 7225885 parameters, 0 gradients\n",
      "Adding AutoShape... \n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import cv2\n",
    "\n",
    "\n",
    "cap=cv2.VideoCapture(\"tokyo.mp4\")\n",
    "\n",
    "model = torch.hub.load('ultralytics/yolov5', 'yolov5s', pretrained=True)\n",
    "\n",
    "b=model.names[2] = 'car'\n",
    "c=model.names[0] = 'person'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "ac640d0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "size=416\n",
    "\n",
    "count=0\n",
    "counter=0\n",
    "\n",
    "\n",
    "color=(0,0,255)\n",
    "cx1=300\n",
    "cy1=450\n",
    "x_offset=6\n",
    "y_offset=6\n",
    "while True:\n",
    "    ret,img=cap.read()\n",
    "\n",
    "    count += 1\n",
    "    if count % 4 != 0:\n",
    "         continue\n",
    "    img=cv2.resize(img,(900,600)) #prendre pourcentage des pixels taille de l'écran 960,540\n",
    "    cv2.line(img,(cx1-100,cy1),(cx1,cy1-20),(0,0,255),2)\n",
    "\n",
    "    results=model(img,size)\n",
    "    a=results.pandas().xyxy[0]\n",
    "    #print(a)\n",
    "\n",
    "    for index, row in results.pandas().xyxy[0].iterrows():\n",
    "        \n",
    "        x1=int(row['xmin'])\n",
    "        y1=int(row['ymin'])\n",
    "        x2=int(row['xmax'])\n",
    "        y2=int(row['ymax'])\n",
    "        \n",
    "        x_dist=x2-x1\n",
    "        y_dist=y2-y1\n",
    "        sum_dist=x_dist+y_dist\n",
    "        \n",
    "        d=(row['class'])\n",
    "        conf=(row['confidence'])\n",
    "        \n",
    "        if d==2:\n",
    "            #print(\"x_dist+ \",x_dist,\"y_dist+ \",y_dist,\"sum\",sum_dist)\n",
    "            if sum_dist>140: #filtrer les rectangles inutiles\n",
    "                if conf>0.4 :\n",
    "                    cv2.rectangle(img,(x1,y1),(x2,y2),(0,255,0),2)\n",
    "                    rectx1,recty1=((x1+x2)/2,(y1+y2)/2)\n",
    "                    rectcenter=int(rectx1),int(recty1)\n",
    "                    cx=rectcenter[0]\n",
    "                    cy=rectcenter[1]\n",
    "                    cv2.circle(img,(cx,cy),2,(255,0,0),-1)\n",
    "                    #cv2.putText(img,str(conf),(x1,y1),cv2.FONT_HERSHEY_PLAIN,2,(255,255,255),2)\n",
    "                #if cy<(cy1+offset) and cy>(cy1-offset) and cx<(cx1+offset) and cx>(cx1-offset):\n",
    "                if cx<(cx1+x_offset) and cx>(cx1-x_offset) and cy<(cy1+y_offset):\n",
    "                    counter+=1\n",
    "                    cv2.line(img,(cx1-100,cy1),(cx1,cy1-20),(0,255,0),3)\n",
    "                    print(counter)\n",
    "                    #cv2.putText(img,str(counter),(x2,y2),cv2.FONT_HERSHEY_PLAIN,2,(255,255,255),2)\n",
    "        if d==0:\n",
    "            cv2.rectangle(img,(x1,y1),(x2,y2),(0,0,255),2)\n",
    "            rectx1,recty1=((x1+x2)/2,(y1+y2)/2)\n",
    "            rectcenter=int(rectx1),int(recty1)\n",
    "            cx=rectcenter[0]\n",
    "            cy=rectcenter[1]\n",
    "            cv2.circle(img,(cx,cy),3,(255,0,0),-1)\n",
    "            #cv2.putText(img,str(c),(x1,y1),cv2.FONT_HERSHEY_PLAIN,2,(255,0,0),2)\n",
    "    cv2.imshow(\"tokyo\",img)\n",
    "    if cv2.waitKey(1) &0xFF==27: #== ord('q'): #echap sortir\n",
    "        break\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b3f77c9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
